# PCDASNet: Post-disaster Change Detection and Assessment System

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![TensorFlow](https://img.shields.io/badge/TensorFlow-2.x-orange.svg)](https://www.tensorflow.org/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

A two-stage deep learning system for automated post-disaster damage assessment using satellite imagery, designed to support faster emergency response and situational awareness.

---

## Project Overview

PCDASNet (Post-disaster Change Detection and Assessment System Network) implements a sophisticated pipeline for analyzing satellite imagery before and after natural disasters. The system automatically identifies damaged buildings and classifies the severity of damage, enabling rapid assessment for disaster response teams.

### Key Features

- **Two-Stage Architecture**: Sequential building localization and damage classification
- **Siamese U-Net**: Parallel processing of pre- and post-disaster imagery with shared encoder
- **Differential Attention Module (DAM)**: CBAM and SSAM for enhanced bi-temporal feature discrimination
- **Advanced Post-processing**: SLIC superpixels, morphological operations, and instance-level voting
- **GPU-Optimized**: Mixed precision training and optimized data pipelines

---

##  System Architecture

### Overall Pipeline Diagram

```STAGE 1: BUILDING EXTRACTION
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Pre-disaster Image
        â†“
Encoderâ€“Decoder Network (U-Net style)
        â†“
Binary Building Mask (P_b)
        â†“
Morphological Dilation (MD)
        â†“
Position Mask (P_B = MD(P_b))
        â”‚
        â”‚  (used as prior in Stage 2)
        â–¼


STAGE 2: BUILDING DAMAGE CLASSIFICATION (SIAMESE)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Pre-disaster Image (+ SLIC)        Post-disaster Image (+ SLIC)
        â”‚                                   â”‚
        â–¼                                   â–¼
   Siamese Encoder (shared weights, pretrained from Stage 1)
        â”‚
        â–¼
 Multi-level Encoder Features (Pre & Post)
        â”‚
        â–¼
 (Differential Attention applied at EACH skip level)
        â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SHALLOW LEVELS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                                                    â”‚
        â–¼                                                    â–¼
  Shallow Differential Attention Module (S-DAM)        Deep Differential Attention Module (D-DAM)

  â€¢ Feature Difference: (F_pre âˆ’ F_post)               â€¢ Feature Difference: (F_pre âˆ’ F_post)
  â€¢ CBAM (Channel + Spatial Attention)                 â€¢ CBAM (Channel + Spatial Attention)
  â€¢ Position Constraint using P_B                      â€¢ NO position constraint
  â€¢ SSAM applied ONLY to pre-disaster branch            â€¢ NO SSAM
        â”‚                                                    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Skip Connections â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             to Decoder
                                â†“
                      Siamese Decoder
                                â†“
                  Pixel-wise Damage Prediction (P_d)
                                â†“
          Superpixel-based Post-Processing (SPP)
                                â†“
               Refined Damage Map (P_D)
                                â†“
        Final Output:  P = P_B âŠ™ P_D   (Masked Damage Map)

```

### Stage 1: Building Extraction (Localization Network)

**Input:** Pre-disaster image I_pre âˆˆ R^(3Ã—HÃ—W)

**Network:** Encoder-decoder (U-Net-like) backbone with skip connections

**Output:** Binary building mask P_b âˆˆ {0,1}^(1Ã—HÃ—W)

**Mask Expansion:**
```
P_B = MD(P_b)  ... Equation (1)
```
Morphological dilation to include surrounding environment. This expanded mask represents building + nearby context and is used as prior knowledge in Stage 2.

### Stage 2: Building Damage Classification (Siamese Network)

**Inputs:**
- Pre-disaster image I_pre
- Post-disaster image I_post
- Superpixel-augmented versions of both images (via SLIC)
- Expanded building mask P_B from Stage 1

**Network:** Siamese encoder-decoder with weights initialized from Stage 1, featuring:
- Dual branches for pre-disaster & post-disaster processing
- Differential Attention Module (DAM) in skip connections
- Shared encoder weights

**Intermediate Output:** Pixel-wise damage prediction P_d âˆˆ {0,1,2,3}^(1Ã—HÃ—W)

**Post-processing:** Superpixel-based voting
```
P_D = SPP(P_d)  ... Equation (2)
```

**Final Output:** Combined localization + damage classification
```
P = P_B âŠ™ P_D  ... Equation (3)
```

---

## ğŸ”¬ Differential Attention Module (DAM)

DAM explicitly models bi-temporal change features and is guided by building position masks.

### Shallow DAM (High Resolution Features)

**Step 1: Feature Difference**
```
F_DIFF,1^S = F_pre^S - F_post^S  ... Equation (4)
```

**Step 2: Channel + Spatial Attention (CBAM)**
```
F_DIFF,2^S = CBAM(F_DIFF,1^S)  ... Equation (5)
```

**Step 3: Position-Constrained Attention** (from Stage 1 mask)

Attention map:
```
A = Î£_j exp(W_q * P_B,j) / Î£_m exp(W_q * P_B,m)  ... Equation (6)
```

Final constrained differential feature:
```
F_DIFF,3^S = A(W_k * F_DIFF,2^S) + (W_v * F_DIFF,2^S)  ... Equation (7)
```

### Simple Self-Attention Module (SSAM)

Applied only to pre-disaster shallow features:

**Attention:**
```
A_pre = Î£_j exp(W_q * F_pre,j^S) / Î£_m exp(W_q * F_pre,m^S)  ... Equation (8)
```

**Enhanced pre-disaster feature:**
```
FÌ‚_pre^S = A_pre(W_k * F_pre^S) + (W_v * F_pre^S)  ... Equation (9)
```

### Deep DAM (Low Resolution Features)

No position constraint, no SSAM, only CBAM on feature differences:
```
F_DIFF^D = CBAM(F_pre^D - F_post^D)  ... Equation (10)
```

---

## ğŸ“Š Dataset

**xView2 Challenge Dataset**

Download the dataset from: **[https://xview2.org/download](https://xview2.org/download)**

- **Tier3 (Training)**: 6,369 pre-disaster image pairs
- **Test Set**: 933 image pairs
- **Resolution**: 512Ã—512 pixels
- **Damage Classes**: 4 levels
  - **Class 0**: No damage
  - **Class 1**: Minor damage
  - **Class 2**: Major damage
  - **Class 3**: Destroyed

---

##  Usage

### Stage 1: Building Localization

```python
# Setup GPU configuration
from utils.gpu_config import setup_gpu, enable_mixed_precision
setup_gpu()
enable_mixed_precision()

# Load and preprocess data
from data.data_pipeline import create_dataset
dataset = create_dataset(image_files, mask_files, batch_size=4)

# Train Stage 1 model
from models.unet import unet_model
from utils.losses import combined_loss

model = unet_model(input_size=(512, 512, 3))
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss=combined_loss,
    metrics=["accuracy", f1_score_loc, xview2_score]
)

callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='f1_score_loc', 
        patience=5, 
        mode='max', 
        restore_best_weights=True
    ),
    tf.keras.callbacks.ReduceLROnPlateau(
        monitor='loss', 
        factor=0.5, 
        patience=3, 
        verbose=1
    )
]

history = model.fit(dataset, epochs=20, callbacks=callbacks)

# Save encoder weights for Stage 2
from models.unet import encoder_model
encoder = encoder_model()
for layer in encoder.layers:
    if layer.name in [l.name for l in model.layers]:
        try:
            layer.set_weights(model.get_layer(layer.name).get_weights())
        except:
            pass

encoder.save_weights('stage1_encoder_weights.weights.h5')
```

### Stage 2: Damage Classification

```python
# Load pretrained encoder
from models.siamese_unet import encoder_model, siamese_unet_damage_model
shared_enc = encoder_model()
shared_enc.load_weights('stage1_encoder_weights.weights.h5')

# Create Stage 2 dataset with GPU optimization
from data.preprocessing import create_optimized_dataset
stage2_dataset = create_optimized_dataset(
    pre_files, post_files, mask_files, batch_size=4
)

# Build and compile damage classification model
damage_model = siamese_unet_damage_model()
damage_model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss=combined_loss_multiclass,
    metrics=['accuracy', f1_cls0, f1_cls1, f1_cls2, f1_cls3, xview2_score_damage]
)

# Train Stage 2
history_stage2 = damage_model.fit(
    stage2_dataset,
    epochs=20,
    steps_per_epoch=len(pre_files) // batch_size,
    callbacks=callbacks
)

damage_model.save('stage2_damage_model.keras')
```

### Inference

```python
# Load trained models
stage1_model = tf.keras.models.load_model('stage1_model.keras')
stage2_model = tf.keras.models.load_model('stage2_damage_model.keras')

# Predict building masks
building_mask = stage1_model.predict(pre_image)

# Apply morphological dilation
from utils.postprocessing import apply_morphological_dilation
expanded_mask = apply_morphological_dilation(building_mask, dilation_pixels=10)

# Predict damage classification
damage_pred = stage2_model.predict([pre_image, post_image, expanded_mask])

# Apply post-processing
from utils.postprocessing import superpixel_postprocessing, instance_voting
refined_pred = superpixel_postprocessing(damage_pred, pre_segments, building_mask)
final_pred = instance_voting(refined_pred, building_instances)

# Visualize results
import matplotlib.pyplot as plt
plt.figure(figsize=(15, 5))
plt.subplot(1, 4, 1); plt.imshow(pre_image); plt.title('Pre-disaster')
plt.subplot(1, 4, 2); plt.imshow(post_image); plt.title('Post-disaster')
plt.subplot(1, 4, 3); plt.imshow(expanded_mask[..., 0], cmap='gray'); plt.title('Building Mask')
plt.subplot(1, 4, 4); plt.imshow(final_pred); plt.title('Damage Classification')
plt.show()
```

---

## ğŸ“ Loss Functions

### Stage 1: Building Extraction Loss
```
L_1 = Î»_1 * L_dice + Î»_2 * L_focal  ... Equation (11)
```
where Î»_1 = 0.3, Î»_2 = 0.7

### Stage 2: Damage Classification Loss
```
L_2 = Î»_1 * L_dice + Î»_2 * L_focal + Î»_3 * L_ce  ... Equation (12)
```
where Î»_1 = 0.3, Î»_2 = 0.3, Î»_3 = 0.4

**Loss Components:**
- **Dice Loss**: Addresses class imbalance in segmentation
- **Focal Loss**: Focuses on hard-to-classify pixels
- **Cross-Entropy Loss**: Standard multi-class classification loss

---

## ğŸ“ˆ Results

### âš ï¸ Important Note on Training Limitations

Due to the very large size of high-resolution satellite imagery and **limited local computational resources**, both Stage-1 (change localization) and Stage-2 (instance-level refinement) models were trained for only **1â€“2 epochs**.

**As a result:**
- The experiments primarily validate **pipeline correctness, architectural design, and training stability**
- The obtained outputs should be treated as **proof-of-concept results**, not final performance benchmarks
- Full-scale training with adequate GPU resources (multi-epoch training and hyperparameter tuning) is expected to significantly improve quantitative metrics and qualitative mask quality

### Current Performance (Proof of Concept)

#### Stage 1: Building Localization
| Metric | Value |
|--------|-------|
| Test Accuracy | 93.26% |
| F1 Score (Localization) | 0.3537 |
| Loss | 1.0218 |

#### Stage 2: Damage Classification
| Metric | Value |
|--------|-------|
| Training Epochs | 2 (limited) |
| Batch Size | 2 (GPU memory constraints) |
| Damage Classes | 4 (No damage, Minor, Major, Destroyed) |
| F1 Scores | Early training phase |
| xView2 Score | Under development |

### Qualitative Results
The system successfully demonstrates:
- âœ… Accurate building boundary detection with morphological refinement
- âœ… Superpixel-based damage region segmentation
- âœ… Instance-level consistency in damage classification
- âœ… GPU-optimized training pipeline functionality
- âœ… Differential attention capturing bi-temporal changes

---

## ğŸ”¬ Technical Contributions

1. **Two-Stage Architecture with Transfer Learning**: 
   - Stage 1 encoder weights initialize Stage 2 Siamese network
   - Enables efficient feature extraction for damage classification

2. **Differential Attention Module (DAM)**:
   - Shallow DAM with position-constrained attention guided by building masks
   - Deep DAM for semantic-level feature differences
   - SSAM for enhanced pre-disaster feature representation

3. **Superpixel Integration**:
   - Input-level: SLIC preprocessing for texture-aware features
   - Output-level: Superpixel voting for spatially coherent predictions

4. **GPU Optimization**:
   - Mixed precision training (FP16)
   - Optimized data pipeline with prefetching
   - TensorFlow graph mode for maximum throughput

5. **Multi-Component Loss Engineering**:
   - Balanced combination of Dice, Focal, and Cross-Entropy losses
   - Addresses class imbalance and hard sample mining

---

## ğŸ“Š Evaluation Metrics

Following the xView2 challenge protocol:

### Localization Metric (F1loc)
Measures building detection accuracy from Stage 1

### Classification Metrics (F1cls)
Per-class F1 scores for damage levels:
- F1clsâ‚€: No damage
- F1clsâ‚: Minor damage
- F1clsâ‚‚: Major damage  
- F1clsâ‚ƒ: Destroyed

### Overall xView2 Score
```
xView2_Score = 0.3 Ã— F1loc + 0.7 Ã— mean(F1clsâ‚€, F1clsâ‚, F1clsâ‚‚, F1clsâ‚ƒ)
```

---

## ğŸ”® Future Work

- [ ] **Full-Scale Training**: Extended epochs (50+) on multi-GPU cluster
- [ ] **Hyperparameter Optimization**: Grid search for loss weights, learning rates
- [ ] **Temporal Attention**: Multi-temporal analysis for disaster progression
- [ ] **Multi-Scale Processing**: Feature pyramid networks for varying building sizes
- [ ] **Real-Time Inference**: Model quantization and TensorRT optimization
- [ ] **Additional Disasters**: Extend to floods, fires, earthquakes
- [ ] **Weakly-Supervised Learning**: Reduce annotation requirements
- [ ] **Uncertainty Quantification**: Bayesian deep learning for confidence estimates
- [ ] **Active Learning**: Iterative model improvement with human feedback

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for bugs and feature requests.

---

## ğŸ‘¥ Authors

**Abdul Hadi Zeeshan**
- GitHub: [@ahz002](https://github.com/AHZ002)
- LinkedIn: [Abdul Hadi](https://www.linkedin.com/in/abdul-hadi-070727259/)
- Email: abdulhadizeeshan79@gmail.com

---

## ğŸ™ Acknowledgments

- **xView2 Challenge** organizers for the comprehensive dataset ([https://xview2.org/](https://xview2.org/))
- TensorFlow and Keras teams for excellent deep learning frameworks
- The computer vision research community for foundational work on U-Net and attention mechanisms
- Open-source contributors for tools like SLIC, scikit-image, and OpenCV

---

## ğŸ“š References

1. **Gupta, R., et al. (2019)**. "xBD: A Dataset for Assessing Building Damage from Satellite Imagery." *CVPR Workshop on Computer Vision for Global Challenges*.

2. **Ronneberger, O., Fischer, P., & Brox, T. (2015)**. "U-Net: Convolutional Networks for Biomedical Image Segmentation." *MICCAI*.

3. **Woo, S., Park, J., Lee, J. Y., & Kweon, I. S. (2018)**. "CBAM: Convolutional Block Attention Module." *ECCV*.

4. **Achanta, R., et al. (2012)**. "SLIC Superpixels Compared to State-of-the-art Superpixel Methods." *IEEE TPAMI*.

5. **Lin, T. Y., et al. (2017)**. "Focal Loss for Dense Object Detection." *ICCV*.

6. **Daudt, R. C., et al. (2018)**. "Fully Convolutional Siamese Networks for Change Detection." *ICIP*.
---

<div align="center">

**â­ Star this repository if you find it helpful!**

**ğŸŒ Built for disaster response and humanitarian applications**

[![GitHub stars](https://img.shields.io/github/stars/yourusername/PCDASNet?style=social)](https://github.com/yourusername/PCDASNet)
[![GitHub forks](https://img.shields.io/github/forks/yourusername/PCDASNet?style=social)](https://github.com/yourusername/PCDASNet/fork)

</div>
